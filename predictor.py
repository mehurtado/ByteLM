# predictor.py

import torch
import torch.nn.functional as F # For softmax

# Assuming model.py and config.py are in the same directory or accessible
from model import ByteLLM_GrugV3
from config import CONFIG_V3 # For default generation/model parameters in tests

class Predictor:
    """
    Handles sequence generation using a trained ByteLLM_GrugV3 model.
    """
    def __init__(self, model: ByteLLM_GrugV3, device: torch.device,
                 generation_config: dict, model_internal_config: dict):
        """
        Initializes the Predictor.

        Args:
            model (ByteLLM_GrugV3): The trained model instance.
            device (torch.device): The device to run generation on (e.g., 'cuda', 'cpu').
            generation_config (dict): Configuration for generation, e.g.,
                                      {"generation_temperature": 1.0, "generation_top_k": 0}.
            model_internal_config (dict): Configuration related to the model's architecture
                                          that affects prediction, e.g.,
                                          {"max_positional_encoding_len": 512, "sequence_length": 16}.
                                          'sequence_length' here refers to the training context length.
        """
        self.model = model.to(device).eval() # Ensure model is on correct device and in eval mode
        self.device = device
        
        self.temperature = generation_config.get("generation_temperature", 1.0)
        self.top_k = generation_config.get("generation_top_k", 0) # 0 means no top-k filtering

        # These are crucial for managing the input to the model during generation
        self.model_context_len = model_internal_config.get("sequence_length", 16) # From training
        self.max_model_input_len = model_internal_config.get("max_positional_encoding_len", 512)

        if self.temperature <= 0:
            raise ValueError("Temperature must be positive.")
        if self.top_k < 0:
            raise ValueError("top_k must be non-negative.")
        if self.model_context_len <= 0:
            raise ValueError("model_internal_config 'sequence_length' (model_context_len) must be positive.")
        if self.max_model_input_len <= 0:
             raise ValueError("model_internal_config 'max_positional_encoding_len' must be positive.")
        if self.model_context_len > self.max_model_input_len:
            print(f"Warning: model_context_len ({self.model_context_len}) from training is greater than "
                  f"max_model_input_len ({self.max_model_input_len}). Effective context will be capped by max_model_input_len.")
            # Effective context length will be min(self.model_context_len, self.max_model_input_len)

        print(f"Predictor initialized: Temp={self.temperature}, TopK={self.top_k}, "
              f"ModelContextLen={self.model_context_len}, MaxModelInputLen={self.max_model_input_len}")

    @torch.no_grad() # Disable gradient calculations during generation
    def generate_sequence(self, seed_bytes: bytes | list[int], length: int = 100) -> bytes:
        """
        Generates a sequence of bytes starting from seed_bytes.

        Args:
            seed_bytes (bytes | list[int]): The initial sequence of bytes (or their integer values)
                                            to start generation from.
            length (int): The number of new bytes to generate.

        Returns:
            bytes: The seed_bytes followed by the newly generated bytes.
        """
        self.model.eval() # Ensure model is in evaluation mode

        if isinstance(seed_bytes, bytes):
            current_sequence_values = list(seed_bytes)
        elif isinstance(seed_bytes, list) and all(isinstance(x, int) and 0 <= x <= 255 for x in seed_bytes):
            current_sequence_values = list(seed_bytes)
        else:
            raise ValueError("seed_bytes must be actual bytes or a list of integers (0-255).")

        if length <= 0:
            return bytes(current_sequence_values)

        generated_continuation = []

        for _ in range(length):
            # Determine the actual context to feed to the model
            # It should be at most self.model_context_len (training sequence length)
            # and also capped by self.max_model_input_len (model's absolute max PE length)
            effective_context_window = min(self.model_context_len, self.max_model_input_len)
            
            start_idx = max(0, len(current_sequence_values) - effective_context_window)
            input_sequence_for_model = current_sequence_values[start_idx:]

            # This check should ideally not be needed if effective_context_window is managed well,
            # but as a safeguard:
            if len(input_sequence_for_model) > self.max_model_input_len:
                 input_sequence_for_model = input_sequence_for_model[-self.max_model_input_len:]

            if not input_sequence_for_model:
                # Handle empty seed case: start with a common padding token or a random one.
                # For a byte model, 0 (often NULL) is a possibility.
                print("Warning: Generating from an empty or very short seed. Using [0] as initial input.")
                input_tensor = torch.tensor([[0]], dtype=torch.long, device=self.device)
            else:
                input_tensor = torch.tensor([input_sequence_for_model], dtype=torch.long, device=self.device)
            
            logits = self.model(input_tensor) # (1, vocab_size)
            
            # Apply temperature
            logits_scaled = logits / self.temperature

            # Apply top-k filtering
            if self.top_k > 0:
                # Ensure k is not larger than vocab_size
                k = min(max(1, self.top_k), logits_scaled.size(-1)) 
                top_k_vals, top_k_indices = torch.topk(logits_scaled, k, dim=-1)
                
                # Create a new tensor filled with -inf, then scatter top_k_vals
                filtered_logits = torch.full_like(logits_scaled, float('-inf'))
                filtered_logits.scatter_(-1, top_k_indices, top_k_vals)
            else:
                filtered_logits = logits_scaled
            
            probabilities = F.softmax(filtered_logits, dim=-1)
            
            # Handle potential NaN/Inf in probabilities (e.g., from extreme temperatures or model issues)
            if torch.isnan(probabilities).any() or probabilities.sum() < 1e-6 :
                print("Warning: Invalid probabilities encountered during generation (NaN or sum too small). "
                      "Falling back to argmax of scaled logits.")
                # Fallback: use argmax of the scaled (but not top-k filtered, to avoid all -inf) logits
                next_byte_val = torch.argmax(logits_scaled, dim=-1).item()
            else:
                next_byte_val = torch.multinomial(probabilities, 1).item()
            
            current_sequence_values.append(next_byte_val)
            generated_continuation.append(next_byte_val)
        
        return bytes(current_sequence_values) # Returns original seed + generated part

if __name__ == '__main__':
    print("--- Testing predictor.py ---")

    # 1. Setup: Create a dummy model and configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Use a minimal version of CONFIG_V3 for the dummy model
    # Ensure all keys needed by ByteLLM_GrugV3 and Predictor are present
    test_pred_config = CONFIG_V3.copy()
    test_pred_config["embedding_dim"] = 32 # Smaller for faster test
    test_pred_config["attention_d_model"] = 32
    test_pred_config["num_attention_layers"] = 1
    test_pred_config["attention_num_heads"] = 2
    test_pred_config["sequence_length"] = 8        # Training context length
    test_pred_config["max_positional_encoding_len"] = 16 # Max PE length
    test_pred_config["use_cnn_frontend"] = False # Simpler model for testing predictor

    print(f"Test Predictor - Model Config: {test_pred_config}")
    
    try:
        dummy_model = ByteLLM_GrugV3(test_pred_config).to(device)
        print("Dummy model created.")

        # 2. Initialize Predictor
        generation_params = {
            "generation_temperature": 0.8,
            "generation_top_k": 5
        }
        # model_internal_params must match what the dummy_model was configured with for context
        model_internal_params_for_predictor = {
            "sequence_length": test_pred_config["sequence_length"],
            "max_positional_encoding_len": test_pred_config["max_positional_encoding_len"]
        }
        
        print(f"Test Predictor - Generation Config: {generation_params}")
        print(f"Test Predictor - Model Internal Config for Predictor: {model_internal_params_for_predictor}")

        predictor = Predictor(dummy_model, device, generation_params, model_internal_params_for_predictor)
        print("Predictor initialized.")

        # 3. Test generation
        seed_text = "Hello Grug "
        seed_bytes_list = list(seed_text.encode('utf-8'))
        num_bytes_to_generate = 20

        print(f"\nGenerating sequence with seed: '{seed_text}' (bytes: {seed_bytes_list})")
        print(f"Requesting {num_bytes_to_generate} new bytes.")

        generated_output_bytes = predictor.generate_sequence(seed_bytes_list, length=num_bytes_to_generate)
        
        print(f"Generated output (bytes type): {generated_output_bytes}")
        expected_total_length = len(seed_bytes_list) + num_bytes_to_generate
        assert len(generated_output_bytes) == expected_total_length, \
            f"Generated sequence length mismatch. Expected {expected_total_length}, got {len(generated_output_bytes)}"

        try:
            # Attempt to decode the full sequence
            decoded_text = generated_output_bytes.decode('utf-8', errors='replace')
            print(f"Decoded generated text: \"{decoded_text}\"")
        except Exception as e:
            print(f"Could not decode generated bytes: {e}")

        # Test with empty seed
        print("\nGenerating sequence with empty seed...")
        empty_seed_generated = predictor.generate_sequence([], length=10)
        print(f"Generated from empty seed (bytes type): {empty_seed_generated}")
        assert len(empty_seed_generated) == 10

        # Test with top_k = 0 (no filtering)
        print("\nGenerating sequence with top_k = 0...")
        generation_params_no_topk = {"generation_temperature": 0.8, "generation_top_k": 0}
        predictor_no_topk = Predictor(dummy_model, device, generation_params_no_topk, model_internal_params_for_predictor)
        gen_no_topk = predictor_no_topk.generate_sequence(seed_bytes_list, length=5)
        print(f"Generated (no top_k): {gen_no_topk.decode('utf-8', errors='replace')}")
        assert len(gen_no_topk) == len(seed_bytes_list) + 5
        
        print("\n--- predictor.py tests completed successfully (if no assertions failed) ---")

    except Exception as e:
        print(f"\nAn error occurred during predictor.py testing:")
        import traceback
        traceback.print_exc()
