import os
import glob
import subprocess

# --- 1. Read current grug_v3.py ---
with open("grug_v3.py", "r", encoding="utf-8") as f:
    grug_v3_content = f.read()

# --- 2. Modify ByteLLM_GrugV3.forward to accept inference_params ---
model_forward_start_marker = "def forward(self, x: torch.Tensor):"
model_forward_new_signature = "def forward(self, x: torch.Tensor, inference_params=None): # Added inference_params"

if model_forward_start_marker in grug_v3_content:
    grug_v3_content = grug_v3_content.replace(model_forward_start_marker, model_forward_new_signature)
    print("Modified ByteLLM_GrugV3.forward to accept inference_params.")
else:
    print("Warning: ByteLLM_GrugV3.forward signature not found as expected. Trainer integration might have issues.")

# --- 3. Define Trainer and Predictor code (adapted from grug_v2.py logic) ---
# Define as a list of strings to avoid multiline string issues with tools
trainer_predictor_code_lines = [
    "# --- Imports needed for Trainer/Predictor ---",
    "import torch",
    "import torch.optim as optim # For optim.lr_scheduler",
    "from torch.cuda.amp import GradScaler, autocast # For AMP in Trainer",
    "from pathlib import Path # For checkpoint paths",
    "import traceback # For error printing",
    "import time # For profiler",
    "import torch.profiler # For profiler",
    "# from mamba_ssm.utils.generation import InferenceParams # Not needed for GrugV3 Predictor",
    "",
    "# --- Predictor (Adapted for GrugV3 - standard autoregressive generation) ---",
    "class Predictor:",
    "    def __init__(self, model, device, generation_config, model_internal_config):",
    "        self.model = model.to(device).eval()",
    "        self.device = device",
    "        self.temperature = generation_config.get(\"generation_temperature\", 1.0)",
    "        self.top_k = generation_config.get(\"generation_top_k\", 0)",
    "        self.model_internal_config = model_internal_config # For max_len, sequence_length (context)",
    "        if self.temperature <= 0: raise ValueError(\"Temperature must be positive.\")",
    "        print(f\"Predictor initialized for GrugV3: Temp={self.temperature}, TopK={self.top_k}\")",
    "        print(f\"Predictor using model's max_pos_len: {self.model_internal_config.get('max_positional_encoding_len', 'N/A')}\")",
    "        print(f\"Predictor using model's training sequence_length for context: {self.model_internal_config.get('sequence_length', 'N/A')}\")",
    "",
    "    @torch.no_grad()",
    "    def generate_sequence(self, seed_bytes, length=100):",
    "        self.model.eval()",
    "        ",
    "        if isinstance(seed_bytes, bytes):",
    "            current_sequence_values = list(seed_bytes)",
    "        elif isinstance(seed_bytes, list) and all(isinstance(x, int) for x in seed_bytes):",
    "            current_sequence_values = list(seed_bytes)",
    "        else:",
    "            raise ValueError(\"seed_bytes must be bytes or list of ints.\")",
    "",
    "        generated_values = list(current_sequence_values) # Start with seed",
    "        ",
    "        max_len_for_model_input = self.model_internal_config.get('max_positional_encoding_len', 512)",
    "        context_len = self.model_internal_config.get('sequence_length', 16)",
    "",
    "        for _ in range(length):",
    "            start_idx = max(0, len(generated_values) - context_len)",
    "            input_sequence = generated_values[start_idx:]",
    "            ",
    "            if len(input_sequence) > max_len_for_model_input:",
    "                input_sequence = input_sequence[-max_len_for_model_input:]",
    "",
    "            if not input_sequence:",
    "                if not generated_values: # if generated_values itself is empty (e.g. empty seed)",
    "                    input_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)",
    "                else: # This path should ideally not be taken if logic is correct",
    "                    break # Stop if sequence becomes empty somehow mid-generation",
    "            else:",
    "                input_tensor = torch.tensor([input_sequence], dtype=torch.long).to(self.device)",
    "",
    "            logits = self.model(input_tensor)",
    "            logits_scaled = logits / self.temperature",
    "",
    "            if self.top_k > 0:",
    "                k = min(max(1, self.top_k), logits_scaled.size(-1))",
    "                top_k_vals, top_k_indices = torch.topk(logits_scaled, k, dim=-1)",
    "                filtered_logits = torch.full_like(logits_scaled, -float('Inf'))",
    "                filtered_logits.scatter_(-1, top_k_indices, top_k_vals)",
    "            else:",
    "                filtered_logits = logits_scaled",
    "",
    "            probabilities = torch.softmax(filtered_logits, dim=-1)",
    "            ",
    "            if torch.isnan(probabilities).any() or probabilities.sum() < 1e-6:",
    "                print(\"Warning: Invalid probabilities. Using argmax.\")",
    "                next_byte_val = torch.argmax(logits_scaled, dim=-1).item()",
    "            else:",
    "                next_byte_val = torch.multinomial(probabilities, 1).item()",
    "",
    "            generated_values.append(next_byte_val)",
    "        return bytes(generated_values)",
    "",
    "# --- Trainer (Adapted for GrugV3) ---",
    "class Trainer:",
    "    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, device, ",
    "                 checkpoint_dir, model_name, scheduler=None, train_config=None):",
    "        self.model = model",
    "        self.train_dataloader = train_dataloader",
    "        self.val_dataloader = val_dataloader",
    "        self.optimizer = optimizer",
    "        self.criterion = criterion",
    "        self.device = device",
    "        self.checkpoint_dir = Path(checkpoint_dir)",
    "        self.model_name = model_name",
    "        self.scheduler = scheduler",
    "        self.train_config = train_config if train_config else CONFIG_V3",
    "        ensure_dir(self.checkpoint_dir)",
    "        if self.train_config.get(\"enable_profiler\"):",
    "            ensure_dir(self.train_config.get(\"profiler_log_dir\", \"./profiler_logs_grug_v3\"))",
    "",
    "        self.current_config_for_checkpoint = self.train_config",
    "        self.current_global_step = 0",
    "        self.use_amp = self.train_config.get(\"use_amp\", False) and self.device.type == 'cuda'",
    "        self.scaler = GradScaler(enabled=self.use_amp)",
    "        if self.use_amp: print(\"Automatic Mixed Precision (AMP) is ENABLED for training.\")",
    "        else: print(\"Automatic Mixed Precision (AMP) is DISABLED for training.\")",
    "",
    "    def _run_profiler_step(self, profiler_context, epoch_num, batch_idx, inputs, targets):",
    "        inputs, targets = inputs.to(self.device, non_blocking=True), targets.to(self.device, non_blocking=True)",
    "        self.optimizer.zero_grad(set_to_none=True)",
    "        with autocast(enabled=self.use_amp):",
    "            outputs = self.model(inputs)",
    "            loss = self.criterion(outputs, targets)",
    "        self.scaler.scale(loss).backward()",
    "        clip_val = self.train_config.get(\"clip_grad_norm_value\")",
    "        if clip_val is not None and clip_val > 0:",
    "            self.scaler.unscale_(self.optimizer)",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=clip_val)",
    "        self.scaler.step(self.optimizer)",
    "        self.scaler.update()",
    "        if profiler_context: profiler_context.step()",
    "        return loss.item()",
    "",
    "    def _perform_lr_warmup(self):",
    "        if self.train_config.get(\"use_lr_warmup\", False) and \\",
    "           self.current_global_step < self.train_config.get(\"lr_warmup_steps\", 0):",
    "            warmup_steps = self.train_config[\"lr_warmup_steps\"]",
    "            target_lr = self.train_config[\"learning_rate\"]",
    "            init_factor = self.train_config.get(\"lr_warmup_init_factor\", 0.01)",
    "            if warmup_steps == 0: lr_scale = 1.0",
    "            elif self.current_global_step == 0: lr_scale = init_factor",
    "            else: lr_scale = init_factor + (1.0 - init_factor) * (self.current_global_step / warmup_steps)",
    "            lr_scale = min(lr_scale, 1.0)",
    "            for param_group in self.optimizer.param_groups: param_group['lr'] = target_lr * lr_scale",
    "            if self.current_global_step == 0 or (self.current_global_step + 1) % (warmup_steps // 10 if warmup_steps >=10 else 1) == 0 or self.current_global_step == warmup_steps -1 :",
    "                 print(f\"Warmup Step {self.current_global_step+1}/{warmup_steps}, Current LR: {self.optimizer.param_groups[0]['lr']:.2e}\")",
    "        elif self.train_config.get(\"use_lr_warmup\", False) and \\",
    "             self.current_global_step == self.train_config.get(\"lr_warmup_steps\", 0):",
    "            target_lr = self.train_config[\"learning_rate\"]",
    "            for param_group in self.optimizer.param_groups: param_group['lr'] = target_lr",
    "            print(f\"Warmup finished. LR set to target: {target_lr:.2e}\")",
    "",
    "    def run_interim_test(self, epoch_num, batch_idx):",
    '        print(f"\\n--- Interim Test @ Epoch {epoch_num+1}, Batch {batch_idx+1} ---")',
    "        self.model.eval()",
    "        interim_gen_config = {",
    "            \"generation_temperature\": self.train_config.get(\"interim_test_temperature\", 1.0),",
    "            \"generation_top_k\": self.train_config.get(\"interim_test_top_k\", 0)",
    "        }",
    "        model_cfg_for_pred = {",
    "            \"max_positional_encoding_len\": self.train_config[\"max_positional_encoding_len\"],",
    "            \"sequence_length\": self.train_config[\"sequence_length\"],",
    "        }",
    "        interim_predictor = Predictor(self.model, self.device, interim_gen_config, model_cfg_for_pred)",
    "        seed_text = \"The meaning of life is \"",
    "        seed_bytes = seed_text.encode('utf-8')",
    "        print(f\"Seed: '{seed_text}'\")",
    "        generated_bytes = interim_predictor.generate_sequence(seed_bytes, length=128)",
    "        try:",
    "            generated_text = generated_bytes.decode('utf-8', errors='replace')",
    '            print(f"Generated (128 bytes): \\"{generated_text}\\"")',
    "        except Exception as e:",
    "            print(f\"Error decoding generated bytes for interim test: {e}\")",
    "            print(f\"Raw generated bytes: {generated_bytes}\")",
    "        self.model.train()",
    '        print(f"--- End Interim Test ---\\n")',
    "",
    "    def train_epoch(self, epoch_num):",
    "        self.model.train()",
    "        epoch_loss = 0",
    "        num_batches = len(self.train_dataloader)",
    "        if num_batches == 0:",
    "            print(f\"Epoch {epoch_num+1}: Training dataloader is empty. Skipping.\")",
    "            return float('inf')",
    "        profiler_active_this_epoch = (self.train_config.get(\"enable_profiler\", False) and ",
    "                                      epoch_num == self.train_config.get(\"profile_epoch_target\", 0))",
    "        prof_context = None",
    "        if profiler_active_this_epoch:",
    "            print(f\"--- Profiler activated for Training, Epoch {epoch_num+1} ---\")",
    "            p_wait = self.train_config.get(\"profiler_schedule_wait\", 5)",
    "            p_warmup = self.train_config.get(\"profiler_schedule_warmup\", 5)",
    "            p_active = self.train_config.get(\"profiler_schedule_active\", 10)",
    "            p_repeat = self.train_config.get(\"profiler_schedule_repeat\", 1)",
    "            prof_schedule = torch.profiler.schedule(wait=p_wait, warmup=p_warmup, active=p_active, repeat=p_repeat)",
    "            prof_log_dir = self.train_config.get(\"profiler_log_dir\", \"./profiler_logs_grug_v3\")",
    "            ensure_dir(Path(prof_log_dir) / \"train\")",
    "            prof_context = torch.profiler.profile(",
    "                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],",
    "                schedule=prof_schedule,",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(Path(prof_log_dir) / \"train\"),",
    "                record_shapes=True, profile_memory=True, with_stack=True )",
    "            prof_context.start()",
    "        for batch_idx, (inputs, targets) in enumerate(self.train_dataloader):",
    "            self._perform_lr_warmup()",
    "            current_loss = self._run_profiler_step(profiler_context, epoch_num, batch_idx, inputs, targets)",
    "            epoch_loss += current_loss",
    "            self.current_global_step += 1",
    "            if (batch_idx + 1) % self.train_config[\"print_every\"] == 0 or (batch_idx + 1) == num_batches:",
    "                print(f\"Epoch {epoch_num+1}/{self.train_config['num_epochs']}, Batch {batch_idx+1}/{num_batches}, Train Loss: {current_loss:.4f}, Current LR: {self.optimizer.param_groups[0]['lr']:.2e}\")",
    "            test_interval = self.train_config.get(\"test_every_batches\", 0)",
    "            if test_interval > 0 and (self.current_global_step % test_interval == 0) and self.current_global_step > 0:",
    "                self.run_interim_test(epoch_num, batch_idx)",
    "        if prof_context:",
    "            prof_context.stop()",
    "            print(f\"--- Profiler stopped for Training, Epoch {epoch_num+1} ---\")",
    "            print(f\"Training Profiler traces saved to: {Path(prof_log_dir) / 'train'}\")",
    "        if self.device.type == 'cuda': torch.cuda.empty_cache()",
    "        return epoch_loss / num_batches if num_batches > 0 else float('inf')",
    "",
    "    def evaluate_epoch(self, epoch_num):",
    "        self.model.eval()",
    "        val_loss = 0",
    "        if not self.val_dataloader:",
    "            print(f\"Epoch {epoch_num+1}: Validation dataloader is not available. Skipping validation.\")",
    "            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):",
    "                is_after_warmup = not self.train_config.get(\"use_lr_warmup\",False) or self.current_global_step >= self.train_config.get(\"lr_warmup_steps\",0)",
    "                if is_after_warmup: self.scheduler.step()",
    "            return float('inf')",
    "        num_val_batches = len(self.val_dataloader)",
    "        if num_val_batches == 0:",
    "            print(f\"Epoch {epoch_num+1}: Validation dataloader is empty. Skipping validation.\")",
    "            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):",
    "                is_after_warmup = not self.train_config.get(\"use_lr_warmup\",False) or self.current_global_step >= self.train_config.get(\"lr_warmup_steps\",0)",
    "                if is_after_warmup: self.scheduler.step()",
    "            return float('inf')",
    "        profiler_active_this_epoch = (self.train_config.get(\"enable_profiler\", False) and ",
    "                                      epoch_num == self.train_config.get(\"profile_epoch_target\", 0))",
    "        prof_context_eval = None",
    "        if profiler_active_this_epoch:",
    "            print(f\"--- Profiler activated for Validation, Epoch {epoch_num+1} ---\")",
    "            p_active_eval = min(5, num_val_batches)",
    "            prof_log_dir = self.train_config.get(\"profiler_log_dir\", \"./profiler_logs_grug_v3\")",
    "            ensure_dir(Path(prof_log_dir) / \"eval\")",
    "            prof_context_eval = torch.profiler.profile(",
    "                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],",
    "                schedule=torch.profiler.schedule(wait=0, warmup=0, active=p_active_eval, repeat=1),",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(Path(prof_log_dir) / \"eval\"),",
    "                record_shapes=True, profile_memory=True, with_stack=True )",
    "            prof_context_eval.start()",
    "        with torch.no_grad():",
    "            for batch_idx_eval, (inputs, targets) in enumerate(self.val_dataloader):",
    "                inputs, targets = inputs.to(self.device, non_blocking=True), targets.to(self.device, non_blocking=True)",
    "                with autocast(enabled=self.use_amp):",
    "                    outputs = self.model(inputs)",
    "                    loss = self.criterion(outputs, targets)",
    "                val_loss += loss.item()",
    "                if prof_context_eval and batch_idx_eval < p_active_eval: prof_context_eval.step()",
    "        if prof_context_eval:",
    "            prof_context_eval.stop()",
    "            print(f\"--- Profiler stopped for Validation, Epoch {epoch_num+1} ---\")",
    "            print(f\"Validation Profiler traces saved to: {Path(prof_log_dir) / 'eval'}\")",
    "        avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else float('inf')",
    "        print(f\"Epoch {epoch_num+1}/{self.train_config['num_epochs']}, Validation Loss: {avg_val_loss:.4f}\")",
    "        if self.scheduler:",
    "            is_after_warmup = not self.train_config.get(\"use_lr_warmup\",False) or self.current_global_step >= self.train_config.get(\"lr_warmup_steps\",0)",
    "            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau): self.scheduler.step(avg_val_loss)",
    "            elif is_after_warmup: self.scheduler.step()",
    "        self.model.train()",
    "        if self.device.type == 'cuda': torch.cuda.empty_cache()",
    "        return avg_val_loss",
    "",
    "    def train(self, num_epochs):",
    "        print(f\"Starting GrugV3 training with model {self.model_name}...\"); self.model.to(self.device)",
    "        start_epoch = 0; best_val_loss = float('inf')",
    "        loaded_info = self.load_checkpoint(self.train_config.get(\"resume_from_checkpoint\"))",
    "        if loaded_info:",
    "            loaded_epoch = loaded_info.get('epoch', -1)",
    "            self.current_global_step = loaded_info.get('current_global_step', 0)",
    "            print(f\"Resuming GrugV3 training from epoch {loaded_epoch + 1}. Global step set to {self.current_global_step}\")",
    "            start_epoch = loaded_epoch + 1",
    "            if self.train_config.get(\"reset_best_val_loss_on_resume\", False): best_val_loss = float('inf')",
    "            elif loaded_info.get('loss') is not None: best_val_loss = loaded_info['loss']",
    "            if self.use_amp and 'scaler_state_dict' in loaded_info and loaded_info['scaler_state_dict']:",
    "                try: self.scaler.load_state_dict(loaded_info['scaler_state_dict']); print(\"GradScaler state loaded.\")",
    "                except: print(\"Warning: Could not load GradScaler state.\")",
    "        else: self.current_global_step = 0; print(\"No checkpoint for GrugV3, starting fresh.\")",
    "        for epoch in range(start_epoch, num_epochs):",
    "            avg_train_loss = self.train_epoch(epoch)",
    "            current_val_loss = self.evaluate_epoch(epoch)",
    "            if current_val_loss < best_val_loss:",
    "                best_val_loss = current_val_loss",
    "                print(f\"New best GrugV3 validation loss: {best_val_loss:.4f}. Saving best model...\")",
    "                self.save_checkpoint(epoch, best_val_loss, is_best=True)",
    "            epoch_checkpoint_filename = f\"{self.model_name}_epoch_{epoch+1}.pth\"",
    "            self.save_checkpoint(epoch, current_val_loss, is_best=False, custom_filename=epoch_checkpoint_filename)",
    "        print(\"GrugV3 Training finished.\")",
    "",
    "    def save_checkpoint(self, epoch, val_loss, is_best=False, custom_filename=None):",
    "        checkpoint = { 'epoch': epoch, 'model_state_dict': self.model.state_dict(),",
    "                       'optimizer_state_dict': self.optimizer.state_dict(), 'loss': val_loss,",
    "                       'config': self.current_config_for_checkpoint,",
    "                       'current_global_step': self.current_global_step }",
    "        if self.scheduler: checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()",
    "        if self.use_amp: checkpoint['scaler_state_dict'] = self.scaler.state_dict()",
    "        filename = f\"{self.model_name}_best.pth\" if is_best else custom_filename if custom_filename else f\"{self.model_name}_epoch_{epoch+1}_generic.pth\"",
    "        filepath = self.checkpoint_dir / filename",
    "        torch.save(checkpoint, filepath)",
    "        print(f\"GrugV3 Checkpoint saved to {filepath} (Epoch {epoch+1}, Val Loss: {val_loss:.4f})\")",
    "",
    "    def load_checkpoint(self, specific_checkpoint_path_str=None):",
    "        load_path = None",
    "        if specific_checkpoint_path_str:",
    "            p = Path(specific_checkpoint_path_str)",
    "            if p.is_file(): load_path = p",
    "            else: print(f\"Warning: GrugV3 resume_from_checkpoint path '{p}' not found.\")",
    "        if not load_path:",
    "            if specific_checkpoint_path_str: print(f\"GrugV3 Checkpoint '{specific_checkpoint_path_str}' not found.\")",
    "            return None",
    "        try:",
    "            print(f\"Loading GrugV3 checkpoint from: {load_path}\")",
    "            checkpoint = torch.load(load_path, map_location=self.device)",
    "            chkpt_config = checkpoint.get('config', {})",
    "            if not chkpt_config: print(\"Warning: GrugV3 Checkpoint no config.\")",
    "            elif self.train_config.get(\"use_amp\") != chkpt_config.get(\"use_amp\"): print(\"Warning: AMP setting mismatch in GrugV3 checkpoint.\")",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])",
    "            if 'optimizer_state_dict' in checkpoint and self.optimizer:",
    "                try: self.optimizer.load_state_dict(checkpoint['optimizer_state_dict']); print(\"Optimizer state loaded.\")",
    "                except ValueError as e: print(f\"Warning: Optim state load fail: {e}\")",
    "            if 'scheduler_state_dict' in checkpoint and self.scheduler:",
    "                try: self.scheduler.load_state_dict(checkpoint['scheduler_state_dict']); print(\"Scheduler state loaded.\")",
    "                except Exception as e: print(f\"Warning: Scheduler state load fail: {e}\")",
    "            return { 'epoch': checkpoint.get('epoch', -1), 'loss': checkpoint.get('loss', float('inf')), ",
    "                     'config': chkpt_config, 'current_global_step': checkpoint.get('current_global_step',0),",
    "                     'scaler_state_dict': checkpoint.get('scaler_state_dict') }",
    "        except Exception as e:",
    "            print(f\"Error loading GrugV3 checkpoint {load_path}: {e}\"); traceback.print_exc()",
    "        return None"
]
trainer_predictor_code = "\n".join(trainer_predictor_code_lines)

# --- 4. Insert Trainer and Predictor code into grug_v3.py ---
learnable_pe_marker = "class LearnablePositionalEncoding(nn.Module):"
insertion_point = grug_v3_content.find(learnable_pe_marker)

if insertion_point == -1:
    main_guard_marker = "if __name__ == '__main__':"
    insertion_point = grug_v3_content.find(main_guard_marker)
    if insertion_point == -1:
        print("Warning: Preferred insertion points not found. Appending Trainer/Predictor to end of file.")
        grug_v3_content += f"\n{trainer_predictor_code}\n"
    else:
        grug_v3_content = grug_v3_content[:insertion_point] + trainer_predictor_code + "\n" + grug_v3_content[insertion_point:]
        print("Inserted Trainer/Predictor before __main__ guard.")
else:
    grug_v3_content = grug_v3_content[:insertion_point] + trainer_predictor_code + "\n" + grug_v3_content[insertion_point:]
    print("Inserted Trainer/Predictor before LearnablePositionalEncoding class.")

# --- 5. Write updated content back ---
with open("grug_v3.py", "w", encoding="utf-8") as f:
    f.write(grug_v3_content)
print("Trainer and Predictor classes (adapted for GrugV3) added to grug_v3.py.")

# --- 6. Test execution (syntax check) ---
print("Attempting to execute grug_v3.py to check for syntax errors...")
try:
    result = subprocess.run(["python3", "grug_v3.py"], capture_output=True, text=True, timeout=60)
    if result.returncode == 0:
        print("grug_v3.py executed its __main__ block successfully after adding Trainer/Predictor.")
        if result.stdout: print("Stdout:\n", result.stdout)
        if result.stderr: print("Stderr (potentially warnings):\n", result.stderr)
    else:
        raise Exception(f"grug_v3.py execution failed with return code {result.returncode}.\nStdout: {result.stdout}\nStderr: {result.stderr}")
except subprocess.TimeoutExpired:
    print("grug_v3.py execution timed out.")
    raise
except Exception as e:
    print(f"An error occurred while trying to execute grug_v3.py: {e}")
    raise
print("Executor script v2 finished.")
