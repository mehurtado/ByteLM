GrugV3 Byte-Level Language ModelOverviewGrugV3 is a PyTorch-based byte-level language model for text generation and sequence modeling research. It processes raw UTF-8 byte sequences, enabling it to handle diverse text and encodings.Status: Actively under development; not yet stable. Performance may vary.Core FeaturesByte-Level Processing: Vocabulary-agnostic, operates directly on UTF-8 bytes.Modular & Configurable Architecture: Key parameters (embeddings, CNN, attention) are defined in config.py.Optional CNN Frontend: Employs 1D CNNs for initial sequence processing.Transformer Core: Utilizes multi-head self-attention and learnable positional encodings.Comprehensive Training & Prediction: Orchestrated by main.py, featuring:Checkpointing (save/resume, best model).Learning rate scheduling (Cosine Annealing, ReduceLROnPlateau) and warmup.Automatic Mixed Precision (AMP) support.Robust data processing from .txt files to a memory-mapped byte array.Optional dummy data generation for testing.Integrated PyTorch profiler support.Text generation capabilities.ArchitectureEmbedding Layer: Maps bytes to dense vectors.CNN Frontend (Optional): (model_components.py) Processes embeddings to capture local patterns.Learnable Positional Encoding: (model_components.py) Adds positional context.Multi-Head Attention Layers: (model.py) Stacked Transformer encoders for learning dependencies.Output Layer: Projects to logits over byte values (0-255) for next-byte prediction.DatasetDataProcessor (in dataset.py) reads .txt files from data_dir, concatenates, encodes to UTF-8 bytes, and saves as all_bytes_grug_v3.npy in processed_data_dir.ByteSequenceDataset (in dataset.py) serves sequences for training/validation.ConfigurationAll settings are managed via the CONFIG_V3 dictionary in config.py. This includes paths, model hyperparameters, training parameters, generation settings, and profiling options.Setup & UsagePrerequisitesPython 3.xPyTorch (torch)NumPy (numpy)pip install torch numpy
Data PreparationPlace UTF-8 encoded .txt training files in the data_dir specified in config.py (default: ./dataset/USE).Dummy data is generated if data_dir is empty and generate_dummy_data_if_empty is True.Data is processed into all_bytes_grug_v3.npy on the first run. Set force_reprocess_data: True in config.py to rebuild.RunningExecute the main script:python3 main.py
Control training and prediction phases via DO_TRAINING and DO_PREDICTION flags in config.py.Checkpoints are saved in checkpoint_dir (default: ./checkpoints_grug_v3), with the best model as [model_name]_best.pth.Project Structuremain.py: Main execution script; orchestrates training and prediction.config.py: Contains the CONFIG_V3 dictionary for all settings.utils.py: General utility functions (e.g., ensure_dir, generate_dummy_data).dataset.py: Houses DataProcessor and ByteSequenceDataset for data handling.model_components.py: Defines LearnablePositionalEncoding and CNNFrontend.model.py: Defines the main ByteLLM_GrugV3 model architecture.predictor.py: Contains the Predictor class for sequence generation.trainer.py: Encapsulates the Trainer class for managing the training loop.ContributingContributions are welcome. Please open an issue or submit a pull request.Collaborators: JulesLicenseMIT License. See LICENSE.md.
